{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from pathos.multiprocessing import Pool\n",
    "from functools import partial\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return 2 * (score - true)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return 2 * np.ones(score.shape[0])\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        return 1 / (1 + np.exp(-score))\n",
    "\n",
    "    def g(self,true,score):\n",
    "        pred = self.pred(score)\n",
    "        return pred - true\n",
    "\n",
    "    def h(self,true,score):\n",
    "        pred = self.pred(score)\n",
    "        return pred * (1 - pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    def __init__(\n",
    "        self,\n",
    "        split_feature = None,\n",
    "        split_threshold = None,\n",
    "        left_child = None,\n",
    "        right_child = None,\n",
    "        depth = None,\n",
    "        weight = None\n",
    "        ):\n",
    "        \n",
    "        # store essential information in every tree node\n",
    "        self.split_feature = split_feature\n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.depth = depth\n",
    "        self.weight = weight\n",
    "\n",
    "        self.is_leaf = (self.left_child is None) and (self.right_child is None)\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.int_member = 0\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        \n",
    "        self.root = self .construct_tree(train, g, h, max_depth = self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        n_test = test.shape[0]\n",
    "        result = []\n",
    "        for i in range(n_test):\n",
    "            each = test[i, :]\n",
    "            node = self.root\n",
    "            while not node.is_leaf:\n",
    "                if each[node.split_feature] <= node.split_threshold and node.left_child is not None:\n",
    "                    node = node.left_child\n",
    "                else:\n",
    "                    node = node.right_child\n",
    "            result.append(node.weight)\n",
    "\n",
    "        return np.array(result)\n",
    "\n",
    "    def construct_tree(self, train, g, h, max_depth):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        #TODO\n",
    "        # stopping condisiton 1\n",
    "        weight = -np.sum(g) / (np.sum(h) + self.lamda)\n",
    "        if max_depth == 0 or train.shape[0] < self.min_sample_split:\n",
    "            return TreeNode(weight=weight)\n",
    "        \n",
    "        # # choose subset of features randomly\n",
    "        # n_features = train.shape[1]\n",
    "        # num_features_to_select = int(self.rf * n_features)\n",
    "        # feature_indices = np.random.choice(n_features, num_features_to_select, replace=False)\n",
    "\n",
    "        # find best splitting rule\n",
    "        feature, threshold, gain= self.find_best_decision_rule(train, g, h)\n",
    "        # feature = feature_indices[feature]      # project back\n",
    "\n",
    "        # stopping condition 2\n",
    "        if gain <= 0:\n",
    "            return TreeNode(weight=weight)\n",
    "        \n",
    "        # divide data based on splitting rules\n",
    "        left_indices = train[:,feature] <= threshold\n",
    "        right_indices = train[:,feature] > threshold\n",
    "\n",
    "        # recursive\n",
    "        left_train, left_g, left_h = train[left_indices], g[left_indices], h[left_indices]\n",
    "        right_train, right_g, right_h = train[right_indices], g[right_indices], h[right_indices]\n",
    "\n",
    "        left_child = self.construct_tree(left_train, left_g, left_h, max_depth-1)\n",
    "        right_child = self.construct_tree(right_train, right_g, right_h, max_depth-1)\n",
    "\n",
    "        return TreeNode(split_feature = feature, split_threshold = threshold, \n",
    "                        left_child = left_child, right_child = right_child)\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, treshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        # train n * d\n",
    "        # g: n * 1\n",
    "        # h: n * 1\n",
    "        n_features = train.shape[1]         \n",
    "        feature = None\n",
    "        threshold = None\n",
    "        gain = float('-inf')\n",
    "        \n",
    "        num_features_to_select = int(self.rf * n_features)\n",
    "        feature_indices = (\n",
    "            np.random.choice(n_features, num_features_to_select, replace=False)\n",
    "            if self.rf != 0 else np.arange(n_features)\n",
    "            )\n",
    "        \n",
    "        # def process_feature(feature_idx):\n",
    "        #     feature_column = train[:, feature_idx]\n",
    "        #     return feature_idx, *self.find_threshold(g, h, feature_column)\n",
    "\n",
    "        # multiprocessing\n",
    "        if self.n_threads is not None and self.n_threads > 1:\n",
    "            feature_columns = [train[:, idx] for idx in feature_indices]\n",
    "            inputs = [(g, h, feature_columns[i]) for i in range(len(feature_columns))]\n",
    "        \n",
    "            with Pool(processes=self.n_threads) as pool:\n",
    "                result = pool.starmap(self.find_threshold, inputs)\n",
    "            # with ThreadPoolExecutor(max_workers=self.n_threads) as executor:\n",
    "            #     results = executor.map(process_feature, idx)\n",
    "            \n",
    "            # print(result) [(return value of self.find_threshold), ()...()]\n",
    "            # maybe confusing, just to align with the default return value so I use best_threshold, best_gain here instead of threshold, gain\n",
    "            for feature_idx, (best_threshold, best_gain) in zip(feature_indices, result):\n",
    "                if best_gain > gain:\n",
    "                    gain = best_gain\n",
    "                    threshold = best_threshold\n",
    "                    feature = feature_idx\n",
    "\n",
    "\n",
    "        else:\n",
    "            for feature_idx in feature_indices:\n",
    "                feature_column = train[:,feature_idx]\n",
    "                best_threshold, best_gain = self.find_threshold(g, h, feature_column)\n",
    "                if best_gain > gain:\n",
    "                    gain = best_gain\n",
    "                    threshold = best_threshold\n",
    "                    feature = feature_idx\n",
    "                \n",
    "\n",
    "        return feature, threshold, gain\n",
    "    \n",
    "    def find_threshold(self, g, h, train):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,   \n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        #TODO\n",
    "        # sorting idx\n",
    "        sorted_idx = np.argsort(train)          # eg 100 * 1\n",
    "        sorted_train = train[sorted_idx]\n",
    "        sorted_g = g[sorted_idx]\n",
    "        sorted_h = h[sorted_idx]\n",
    "        best_threshold = None\n",
    "        best_gain = float('-inf')\n",
    "\n",
    "        # gradient and hessian sum of all the nodes before splitting\n",
    "        Gradient_sum = np.sum(sorted_g)\n",
    "        Hessian_sum = np.sum(sorted_h)\n",
    "\n",
    "        # initialize left and right nodes' sum of gradient and hessian\n",
    "        # At the initial state, left node is empty and right node contains all the data\n",
    "        G_left, G_right = 0, Gradient_sum\n",
    "        H_left, H_right = 0, Hessian_sum\n",
    "\n",
    "        for i in range(1, len(sorted_train)):\n",
    "            # gradually move data in the right to left\n",
    "            G_left += sorted_g[i-1]\n",
    "            G_right -= sorted_g[i-1]\n",
    "            H_left += sorted_h[i-1]\n",
    "            H_right -= sorted_h[i-1]\n",
    "\n",
    "            # when we encounter a different feature, we will do the split\n",
    "            if sorted_train[i] != sorted_train[i - 1]:\n",
    "                gain = 0.5 * ((G_left ** 2) / (H_left + self.lamda) + (G_right ** 2) / (H_right + self.lamda) - (Gradient_sum ** 2) / (Hessian_sum + self.lamda)) - self.gamma\n",
    "\n",
    "                if gain > best_gain:\n",
    "                    best_gain = gain\n",
    "                    best_threshold = (sorted_train[i - 1] + sorted_train[i]) / 2  # middle point\n",
    "\n",
    "\n",
    "        return [best_threshold, best_gain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "import pandas as pd\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "\n",
    "        # Ensure train and target are numpy arrays\n",
    "        # if isinstance(train, pd.DataFrame):\n",
    "        #     train = train.to_numpy()\n",
    "        # if isinstance(target, pd.Series):\n",
    "        #     target = target.to_numpy()\n",
    "        \n",
    "\n",
    "\n",
    "        n_samples = train.shape[0]\n",
    "        # bootstrap with replacement\n",
    "        for _ in range(self.num_trees):\n",
    "            idx = np.random.choice(np.arange(n_samples), size=n_samples, replace=True)\n",
    "            # check the idx if valid\n",
    "            # assert idx.max() < n_samples, \"Generated idx contrains out-of-bounds indices\"\n",
    "\n",
    "            sample_train, sample_target = train[idx], target[idx]\n",
    "            \n",
    "            # g, h\n",
    "            pred = np.zeros(sample_target.shape[0])\n",
    "            g, h = self.loss.g(sample_target, pred), self.loss.h(sample_target, pred)\n",
    "            \n",
    "\n",
    "            # train a new tree\n",
    "            tree = Tree(n_threads = self.n_threads, rf = self.rf)\n",
    "            tree.fit(sample_train, g, h)\n",
    "            self.trees.append(tree)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        pred = np.array([tree.predict(test) for tree in self.trees])\n",
    "        score = np.mean(pred, axis=0)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = None, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        # initialize prediction value\n",
    "        # pred = np.zeros(target.shape[0])\n",
    "        pred = np.full(target.shape[0], np.mean(target)) # regression\n",
    "\n",
    "        for _ in range(self.num_trees):\n",
    "            g = self.learning_rate * self.loss.g(target, pred)\n",
    "            h = self.learning_rate ** 2 * self.loss.h(target, pred)\n",
    "\n",
    "            # train a new tree on gradient & hessian\n",
    "            next_tree = Tree(n_threads=self.n_threads, rf = 0)\n",
    "            next_tree.fit(train, g, h)\n",
    "            self.trees.append(next_tree)\n",
    "            pred = self.predict(train)          # consider all the previous trees\n",
    "        # print(pred)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        score = 0\n",
    "        for tree in self.trees:\n",
    "            score += tree.predict(test)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    n = pred.shape[0]\n",
    "    return np.sqrt(np.mean((pred - y) ** 2))\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    #TODO\n",
    "    return sum(pred == y)/ pred.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "<>:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "/var/folders/j4/kk26gmps6pb74rjywjb8nmkc0000gn/T/ipykernel_75414/289120647.py:8: SyntaxWarning: invalid escape sequence '\\s'\n",
      "  raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF rmse_train: 3.4067206922906834    RF rmse_test: 4.190611271956216\n"
     ]
    }
   ],
   "source": [
    "# Problem 1.1\n",
    "# RF for Boston house price dataset\n",
    "model = RF(num_trees = 100, loss=leastsquare())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# results\n",
    "predict_train = model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(predict_test, y_test)\n",
    "print(f\"RF rmse_train: {rmse_train}    RF rmse_test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT rmse_train: 1.7378787449656554    GBDT rmse_test: 3.4593227483389075\n"
     ]
    }
   ],
   "source": [
    "# GBDT for Boston house price dataset\n",
    "model = GBDT(num_trees = 100, learning_rate = 0.001, n_threads = 5, loss=leastsquare())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# results\n",
    "predict_train = model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(predict_test, y_test)\n",
    "print(f\"GBDT rmse_train: {rmse_train}    GBDT rmse_test: {rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least Square rmse_train: 4.820626531838222    Least Square rmse_test: 5.209217510530889\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.6.4\n",
    "# least square for Boston house price dataset\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# linear_model = LinearRegression()\n",
    "# linear_model.fit(X_train, y_train)\n",
    "W = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train)), np.matmul(X_train.T, y_train))\n",
    "\n",
    "# results\n",
    "predict_train = np.matmul(X_train, W)\n",
    "predict_test = np.matmul(X_test, W)\n",
    "linear_rmse_train = root_mean_square_error(predict_train, y_train)\n",
    "linear_rmse_test = root_mean_square_error(predict_test, y_test)\n",
    "print(f\"Least Square rmse_train: {linear_rmse_train}    Least Square rmse_test: {linear_rmse_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ridge regression rmse_train: 4.822434482543473    Ridge Regression rmse_test: 5.186569984437072\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.6.4\n",
    "# ridge regression for Boston house price dataset\n",
    "W = np.matmul(np.linalg.inv(np.matmul(X_train.T, X_train) + 0.5), np.matmul(X_train.T, y_train))\n",
    "\n",
    "# results\n",
    "predict_train = np.matmul(X_train, W)\n",
    "predict_test = np.matmul(X_test, W)\n",
    "ridge_rmse_train = root_mean_square_error(predict_train, y_train)\n",
    "ridge_rmse_test = root_mean_square_error(predict_test, y_test)\n",
    "print(f\"Ridge regression rmse_train: {ridge_rmse_train}    Ridge Regression rmse_test: {ridge_rmse_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,) (700, 20) (700,) (300, 20) (300,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))\n",
    "# X_train.head()\n",
    "\n",
    "# Way 1\n",
    "# preprocess the data\n",
    "\"\"\"Since my model cannot deal with non-numeric data, need to preprocess the dataset\n",
    "Here use a mapping to change the non-numeric data into numeric ones\"\"\"\n",
    "# def prepreprocess_data(X):\n",
    "#     mappings = {}\n",
    "#     non_numeric_cols = X.select_dtypes(exclude='number').columns\n",
    "\n",
    "#     for col in non_numeric_cols:\n",
    "#         # generate mapping rule for each column\n",
    "#         unique = X[col].unique()\n",
    "#         mappings[col] = {each: i / len(unique) - 0.5 for i, each in enumerate(unique)}\n",
    "#         if col in mappings:\n",
    "#             X[col] = X[col].map(mappings[col])\n",
    "#     return X, mappings\n",
    "\n",
    "# Way2\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "def prepreprocess_data(dataframe):\n",
    "    \"\"\"Process the input dataframe by encoding categorical variables and\n",
    "    ensuring numerical columns are integers.\"\"\"\n",
    "    # categorical columns\n",
    "    categorical_cols = [\n",
    "        'checking_status', 'credit_history', 'purpose', 'savings_status',\n",
    "        'employment', 'personal_status', 'other_parties', 'property_magnitude',\n",
    "        'other_payment_plans', 'housing', 'job', 'own_telephone', 'foreign_worker'\n",
    "    ]\n",
    "    \n",
    "    # Encode categorical columns\n",
    "    label_encoders = {}\n",
    "    for col in categorical_cols:\n",
    "        labelencoder = LabelEncoder()\n",
    "        dataframe[col] = labelencoder.fit_transform(dataframe[col])\n",
    "        # label_encoders[col] = labelencoder  # Store encoder for potential inverse transformations later\n",
    "    \n",
    "    # Ensure numerical columns are integers\n",
    "    numerical_cols = dataframe.select_dtypes(include=['int64', 'float64']).columns\n",
    "    for col in numerical_cols:\n",
    "        dataframe[col] = dataframe[col].astype(int)\n",
    "    \n",
    "    return dataframe, label_encoders\n",
    "\n",
    "X, label_encoders = prepreprocess_data(X)\n",
    "\n",
    "\n",
    "# X, mappings = prepreprocess_data(X)\n",
    "# print(X)\n",
    "X = X.values\n",
    "# print(X)\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore the data\n",
    "# print(\"First 5 rows of X:\")\n",
    "# print(X[:5])\n",
    "# print(\"First 5 values of y:\")\n",
    "# print(y[:5])\n",
    "# print(X.columns)\n",
    "# for col in X.columns:\n",
    "#     print(f\"{col} | number of unique values: {X[col].nunique()}\")\n",
    "# print('-----------------------------------------------')\n",
    "# print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train accuracy: 0.76    RF test accuracy: 0.7466666666666667\n"
     ]
    }
   ],
   "source": [
    "# Problem 1.2\n",
    "# RF for Credit-g dataset\n",
    "model = RF(num_trees=100, loss=logistic())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# results\n",
    "predict_train = model.predict(X_train)>0.5\n",
    "accuracy_train = accuracy(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test)>0.5\n",
    "accuracy_test = accuracy(predict_test, y_test)\n",
    "\n",
    "print(f\"RF train accuracy: {accuracy_train}    RF test accuracy: {accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT train accuracy: 0.7585714285714286    GBDT test accuracy: 0.7266666666666667\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.6.5\n",
    "# GBDT for Credit-g dataset\n",
    "model = GBDT(num_trees = 100, learning_rate = 0.003, n_threads = 5, loss=logistic())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# results\n",
    "predict_train = model.predict(X_train)>0.5\n",
    "accuracy_train = accuracy(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test)>0.5\n",
    "accuracy_test = accuracy(predict_test, y_test)\n",
    "\n",
    "print(f\"GBDT train accuracy: {accuracy_train}    GBDT test accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF train accuracy: 0.9849246231155779    RF test accuracy: 0.9649122807017544\n"
     ]
    }
   ],
   "source": [
    "# Problem 1.2\n",
    "# breast cancer with RF\n",
    "model = RF(num_trees=100, loss=logistic())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "predict_train = model.predict(X_train) > threshold\n",
    "accuracy_train = accuracy(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test) > threshold\n",
    "accuracy_test = accuracy(predict_test, y_test)\n",
    "\n",
    "print(f\"RF train accuracy: {accuracy_train}    RF test accuracy: {accuracy_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBDT train accuracy: 0.964824120603015    GBDT test accuracy: 0.9239766081871345\n"
     ]
    }
   ],
   "source": [
    "# Problem 2.6.6\n",
    "# breast cancer with GDBT\n",
    "model = GBDT(num_trees = 100, learning_rate = 0.04, n_threads = 5, loss=logistic())\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# results\n",
    "predict_train = model.predict(X_train)>0.5\n",
    "accuracy_train = accuracy(predict_train, y_train)\n",
    "\n",
    "predict_test = model.predict(X_test)>0.5\n",
    "accuracy_test = accuracy(predict_test, y_test)\n",
    "\n",
    "print(f\"GBDT train accuracy: {accuracy_train}    GBDT test accuracy: {accuracy_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
